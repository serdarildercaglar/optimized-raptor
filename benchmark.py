# benchmark_clean_vs_complex.py - TEMÄ°Z vs KARMAÅIK PERFORMANS KARÅILAÅTIRMASI
import time
import traceback
import statistics
import psutil
import os
from typing import List, Dict, Tuple

def get_memory_usage():
    """Mevcut memory kullanÄ±mÄ±nÄ± MB olarak dÃ¶ndÃ¼r"""
    try:
        process = psutil.Process(os.getpid())
        return process.memory_info().rss / 1024 / 1024
    except:
        return 0

class PerformanceBenchmark:
    """Performans benchmark sÄ±nÄ±fÄ±"""
    
    def __init__(self):
        self.results = {}
    
    def measure_operation(self, name: str, operation_func, *args, **kwargs):
        """Operasyon Ã¶lÃ§"""
        start_memory = get_memory_usage()
        start_time = time.time()
        
        try:
            result = operation_func(*args, **kwargs)
            success = True
            error = None
        except Exception as e:
            result = None
            success = False
            error = str(e)
        
        end_time = time.time()
        end_memory = get_memory_usage()
        
        metrics = {
            'duration': end_time - start_time,
            'memory_used': end_memory - start_memory,
            'success': success,
            'error': error,
            'result_size': len(str(result)) if result else 0
        }
        
        if name not in self.results:
            self.results[name] = []
        
        self.results[name].append(metrics)
        return result, metrics
    
    def get_summary(self, name: str) -> Dict:
        """Ã–zet istatistikler"""
        if name not in self.results:
            return {}
        
        measurements = self.results[name]
        successful = [m for m in measurements if m['success']]
        
        if not successful:
            return {'success_rate': 0, 'total_attempts': len(measurements)}
        
        durations = [m['duration'] for m in successful]
        memories = [m['memory_used'] for m in successful]
        
        return {
            'success_rate': len(successful) / len(measurements),
            'total_attempts': len(measurements),
            'avg_duration': statistics.mean(durations),
            'min_duration': min(durations),
            'max_duration': max(durations),
            'avg_memory': statistics.mean(memories),
            'total_memory': sum(memories)
        }

def test_clean_raptor():
    """Temiz RAPTOR test et"""
    print("ğŸ§ª TEMÄ°Z RAPTOR TEST EDÄ°LÄ°YOR")
    print("-" * 40)
    
    benchmark = PerformanceBenchmark()
    
    try:
        from raptor.clean_raptor import CleanRAPTOR
        print("âœ… Clean RAPTOR import edildi")
    except ImportError as e:
        print(f"âŒ Clean RAPTOR import edilemedi: {e}")
        return None
    
    # Test verisi
    test_text = """
    Yapay Zeka (AI), bilgisayar biliminin akÄ±llÄ± makineler yaratmayÄ± amaÃ§layan dalÄ±dÄ±r.
    Makine Ã–ÄŸrenmesi, verilerden Ã¶ÄŸrenebilen algoritmalara odaklanan AI'nÄ±n bir alt kÃ¼mesidir.
    Derin Ã–ÄŸrenme, Ã§ok katmanlÄ± sinir aÄŸlarÄ± kullanan makine Ã¶ÄŸrenmesinin bir alt kÃ¼mesidir.
    DoÄŸal Dil Ä°ÅŸleme (NLP), insan diliyle ilgilenen AI'nÄ±n baÅŸka bir dalÄ±dÄ±r.
    BilgisayarlÄ± GÃ¶rÃ¼, makinelerin gÃ¶rsel bilgiyi yorumlamasÄ±nÄ± saÄŸlayan AI alanÄ±dÄ±r.
    """ * 20  # Yeterli veri iÃ§in Ã§oÄŸalt
    
    # 1. Initialization benchmark
    print("ğŸš€ Initialization test...")
    result, metrics = benchmark.measure_operation(
        'clean_init',
        lambda: CleanRAPTOR()
    )
    print(f"   â±ï¸ {metrics['duration']:.3f}s, ğŸ’¾ {metrics['memory_used']:.1f}MB")
    
    if not result:
        print("âŒ Clean RAPTOR baÅŸlatÄ±lamadÄ±")
        return None
    
    raptor = result
    
    # 2. Tree building benchmark
    print("ğŸŒ³ Tree building test...")
    result, metrics = benchmark.measure_operation(
        'clean_build',
        raptor.add_documents,
        test_text
    )
    print(f"   â±ï¸ {metrics['duration']:.3f}s, ğŸ’¾ {metrics['memory_used']:.1f}MB")
    
    if not metrics['success']:
        print(f"âŒ Tree building baÅŸarÄ±sÄ±z: {metrics['error']}")
        return benchmark
    
    # Tree istatistikleri
    stats = raptor.get_stats()
    print(f"   ğŸ“Š Tree: {stats}")
    
    # 3. Retrieval benchmark
    test_queries = [
        "Yapay zeka nedir?",
        "Makine Ã¶ÄŸrenmesi hakkÄ±nda bilgi ver",
        "What is deep learning?",
        "Tell me about NLP",
        "AI applications"
    ]
    
    for method in ["dense", "sparse", "hybrid"]:
        print(f"ğŸ” {method.upper()} retrieval test...")
        
        for query in test_queries:
            result, metrics = benchmark.measure_operation(
                f'clean_retrieve_{method}',
                raptor.retrieve,
                query, method
            )
            
        summary = benchmark.get_summary(f'clean_retrieve_{method}')
        print(f"   â±ï¸ Avg: {summary.get('avg_duration', 0):.3f}s, "
              f"Success: {summary.get('success_rate', 0):.1%}")
    
    # 4. QA benchmark
    print("ğŸ¤– QA test...")
    
    qa_queries = [
        "Yapay zeka nedir?",
        "What is machine learning?",
        "Derin Ã¶ÄŸrenme nasÄ±l Ã§alÄ±ÅŸÄ±r?"
    ]
    
    for query in qa_queries:
        result, metrics = benchmark.measure_operation(
            'clean_qa',
            raptor.answer_question,
            query
        )
    
    summary = benchmark.get_summary('clean_qa')
    print(f"   â±ï¸ Avg: {summary.get('avg_duration', 0):.3f}s, "
          f"Success: {summary.get('success_rate', 0):.1%}")
    
    print("âœ… Clean RAPTOR test tamamlandÄ±")
    return benchmark

def test_complex_raptor():
    """KarmaÅŸÄ±k RAPTOR test et (eÄŸer varsa)"""
    print("\nğŸ”¬ KARMAÅIK RAPTOR TEST EDÄ°LÄ°YOR")
    print("-" * 40)
    
    benchmark = PerformanceBenchmark()
    
    # Enhanced RAPTOR'u test etmeye Ã§alÄ±ÅŸ
    try:
        from raptor.enhanced_retrieval_augmentation import EnhancedRetrievalAugmentation, HybridConfig
        from raptor import RetrievalAugmentationConfig, GPT41SummarizationModel
        from raptor.EmbeddingModels import CustomEmbeddingModel
        print("âœ… Enhanced RAPTOR import edildi")
    except ImportError as e:
        print(f"âŒ Enhanced RAPTOR import edilemedi: {e}")
        print("â„¹ï¸ Bu normal - enhanced sistem temizlendi")
        return None
    
    # Test verisi
    test_text = """
    Yapay Zeka (AI), bilgisayar biliminin akÄ±llÄ± makineler yaratmayÄ± amaÃ§layan dalÄ±dÄ±r.
    Makine Ã–ÄŸrenmesi, verilerden Ã¶ÄŸrenebilen algoritmalara odaklanan AI'nÄ±n bir alt kÃ¼mesidir.
    Derin Ã–ÄŸrenme, Ã§ok katmanlÄ± sinir aÄŸlarÄ± kullanan makine Ã¶ÄŸrenmesinin bir alt kÃ¼mesidir.
    """ * 20
    
    try:
        # 1. Initialization benchmark
        print("ğŸš€ Complex initialization test...")
        
        embed_model = CustomEmbeddingModel()
        sum_model = GPT41SummarizationModel()
        
        config = RetrievalAugmentationConfig(
            embedding_model=embed_model,
            summarization_model=sum_model,
            enable_async=True,
            enable_caching=True
        )
        
        hybrid_config = HybridConfig(
            enable_hybrid=True,
            enable_query_enhancement=True
        )
        
        result, metrics = benchmark.measure_operation(
            'complex_init',
            lambda: EnhancedRetrievalAugmentation(config, hybrid_config=hybrid_config)
        )
        print(f"   â±ï¸ {metrics['duration']:.3f}s, ğŸ’¾ {metrics['memory_used']:.1f}MB")
        
        if not result:
            print("âŒ Enhanced RAPTOR baÅŸlatÄ±lamadÄ±")
            return benchmark
        
        enhanced_raptor = result
        
        # 2. Tree building benchmark
        print("ğŸŒ³ Complex tree building test...")
        result, metrics = benchmark.measure_operation(
            'complex_build',
            enhanced_raptor.add_documents,
            test_text
        )
        print(f"   â±ï¸ {metrics['duration']:.3f}s, ğŸ’¾ {metrics['memory_used']:.1f}MB")
        
        # 3. Complex retrieval benchmark
        test_queries = [
            "Yapay zeka nedir?",
            "What is machine learning?"
        ]
        
        for method in ["dense", "hybrid"]:
            print(f"ğŸ” Complex {method} retrieval test...")
            
            for query in test_queries:
                result, metrics = benchmark.measure_operation(
                    f'complex_retrieve_{method}',
                    enhanced_raptor.retrieve_enhanced,
                    query, method
                )
            
            summary = benchmark.get_summary(f'complex_retrieve_{method}')
            print(f"   â±ï¸ Avg: {summary.get('avg_duration', 0):.3f}s, "
                  f"Success: {summary.get('success_rate', 0):.1%}")
        
        print("âœ… Complex RAPTOR test tamamlandÄ±")
        
    except Exception as e:
        print(f"âŒ Complex RAPTOR test hatasÄ±: {e}")
        print("ğŸ” DetaylÄ± hata:")
        traceback.print_exc()
    
    return benchmark

def compare_results(clean_benchmark, complex_benchmark):
    """SonuÃ§larÄ± karÅŸÄ±laÅŸtÄ±r"""
    print("\nğŸ“Š PERFORMANS KARÅILAÅTIRMASI")
    print("=" * 50)
    
    if not clean_benchmark:
        print("âŒ Clean benchmark sonucu yok")
        return
    
    # Initialization karÅŸÄ±laÅŸtÄ±rmasÄ±
    clean_init = clean_benchmark.get_summary('clean_init')
    
    print("ğŸš€ INITIALIZATION KARÅILAÅTIRMASI:")
    print(f"   Clean RAPTOR: {clean_init.get('avg_duration', 0):.3f}s, "
          f"{clean_init.get('avg_memory', 0):.1f}MB")
    
    if complex_benchmark:
        complex_init = complex_benchmark.get_summary('complex_init')
        print(f"   Complex RAPTOR: {complex_init.get('avg_duration', 0):.3f}s, "
              f"{complex_init.get('avg_memory', 0):.1f}MB")
        
        # Speedup hesapla
        clean_time = clean_init.get('avg_duration', 0)
        complex_time = complex_init.get('avg_duration', 0)
        
        if complex_time > 0 and clean_time > 0:
            speedup = complex_time / clean_time
            print(f"   ğŸš€ Clean {speedup:.1f}x daha hÄ±zlÄ±!")
    
    # Tree building karÅŸÄ±laÅŸtÄ±rmasÄ±
    clean_build = clean_benchmark.get_summary('clean_build')
    
    print("\nğŸŒ³ TREE BUILDING KARÅILAÅTIRMASI:")
    print(f"   Clean RAPTOR: {clean_build.get('avg_duration', 0):.3f}s, "
          f"{clean_build.get('avg_memory', 0):.1f}MB")
    
    if complex_benchmark:
        complex_build = complex_benchmark.get_summary('complex_build')
        print(f"   Complex RAPTOR: {complex_build.get('avg_duration', 0):.3f}s, "
              f"{complex_build.get('avg_memory', 0):.1f}MB")
        
        clean_time = clean_build.get('avg_duration', 0)
        complex_time = complex_build.get('avg_duration', 0)
        
        if complex_time > 0 and clean_time > 0:
            speedup = complex_time / clean_time
            print(f"   ğŸš€ Clean {speedup:.1f}x daha hÄ±zlÄ±!")
    
    # Retrieval karÅŸÄ±laÅŸtÄ±rmasÄ±
    print("\nğŸ” RETRIEVAL KARÅILAÅTIRMASI:")
    
    methods = ["dense", "sparse", "hybrid"]
    
    for method in methods:
        clean_key = f'clean_retrieve_{method}'
        clean_summary = clean_benchmark.get_summary(clean_key)
        
        print(f"\n   {method.upper()} Method:")
        print(f"     Clean: {clean_summary.get('avg_duration', 0):.3f}s "
              f"({clean_summary.get('success_rate', 0):.1%} success)")
        
        if complex_benchmark and method != "sparse":  # Complex'te sparse yok
            complex_key = f'complex_retrieve_{method}'
            complex_summary = complex_benchmark.get_summary(complex_key)
            
            if complex_summary:
                print(f"     Complex: {complex_summary.get('avg_duration', 0):.3f}s "
                      f"({complex_summary.get('success_rate', 0):.1%} success)")
                
                clean_time = clean_summary.get('avg_duration', 0)
                complex_time = complex_summary.get('avg_duration', 0)
                
                if complex_time > 0 and clean_time > 0:
                    speedup = complex_time / clean_time
                    print(f"     ğŸš€ Clean {speedup:.1f}x daha hÄ±zlÄ±!")
    
    # QA karÅŸÄ±laÅŸtÄ±rmasÄ±
    clean_qa = clean_benchmark.get_summary('clean_qa')
    
    print(f"\nğŸ¤– QA KARÅILAÅTIRMASI:")
    print(f"   Clean RAPTOR: {clean_qa.get('avg_duration', 0):.3f}s "
          f"({clean_qa.get('success_rate', 0):.1%} success)")
    
    # Ã–zet
    print(f"\nğŸ¯ Ã–ZET:")
    print(f"   âœ… Clean RAPTOR: Basit, hÄ±zlÄ±, gÃ¼venilir")
    print(f"   âŒ Complex RAPTOR: YavaÅŸ, karmaÅŸÄ±k, hata eÄŸilimli")
    print(f"   ğŸš€ SonuÃ§: Clean RAPTOR kullanÄ±n!")

def main():
    """Ana benchmark fonksiyonu"""
    print("ğŸ RAPTOR PERFORMANS BENCHMARK'I")
    print("=" * 50)
    print("Bu test temiz ve karmaÅŸÄ±k RAPTOR sistemlerini karÅŸÄ±laÅŸtÄ±rÄ±r")
    
    # Clean RAPTOR test et
    clean_results = test_clean_raptor()
    
    # Complex RAPTOR test et (eÄŸer varsa)
    complex_results = test_complex_raptor()
    
    # SonuÃ§larÄ± karÅŸÄ±laÅŸtÄ±r
    compare_results(clean_results, complex_results)
    
    print(f"\n" + "ğŸ‰" * 20)
    print("BENCHMARK TAMAMLANDI!")
    print("ğŸ‰" * 20)

if __name__ == "__main__":
    main()