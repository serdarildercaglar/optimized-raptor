"""
DOSYA: generic-qa-server.py
AÃ‡IKLAMA: Jenerik soru-cevap asistanÄ± - RAPTOR RAG sistemi ile entegre WebSocket sunucusu
"""

from dotenv import load_dotenv, find_dotenv
load_dotenv(find_dotenv())

from raptor import RetrievalAugmentation 
from raptor import RetrievalAugmentationConfig
from raptor import GPT41SummarizationModel
from raptor import GPT41QAModel
from raptor.EmbeddingModels import CustomEmbeddingModel

# RAPTOR yapÄ±landÄ±rmasÄ±
embed_model = CustomEmbeddingModel()
sum_model = GPT41SummarizationModel()
qa_model = GPT41QAModel()

RA_config = RetrievalAugmentationConfig(
    tb_summarization_length=100, 
    tb_max_tokens=100, 
    qa_model=qa_model,
    summarization_model=sum_model, 
    embedding_model=embed_model
)

# Ã–nceden oluÅŸturulmuÅŸ RAPTOR tree'sini yÃ¼kle
PATH = "vectordb/raptor-production"
RA = RetrievalAugmentation(tree=PATH, config=RA_config)

from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from typing import Dict, List
import json
import redis.asyncio as redis
import uuid
from openai import AsyncOpenAI
import asyncio

app = FastAPI(title="Generic QA Server", description="RAPTOR-powered Question Answering System")

# Redis yapÄ±landÄ±rmasÄ±
redis_client = redis.Redis(
    host='localhost', 
    port=6379, 
    db=0, 
    password='Ph4nt0m4+4',
    decode_responses=True
)

openai_client = AsyncOpenAI()

# WebSocket baÄŸlantÄ± yÃ¶neticisi
class ConnectionManager:
    def __init__(self):
        self.active_connections: Dict[str, WebSocket] = {}

    async def connect(self, websocket: WebSocket, client_id: str):
        await websocket.accept()
        self.active_connections[client_id] = websocket

    def disconnect(self, client_id: str):
        if client_id in self.active_connections:
            del self.active_connections[client_id]

    async def send_message(self, message: str, client_id: str):
        if client_id in self.active_connections:
            await self.active_connections[client_id].send_text(message)

    async def send_stream_chunk(self, chunk: dict, client_id: str):
        if client_id in self.active_connections:
            await self.active_connections[client_id].send_text(json.dumps(chunk))

manager = ConnectionManager()

# ğŸš€ PARALEL RETRIEVE FONKSÄ°YONU
async def batch_retrieve_async(queries: List[str]) -> Dict[str, any]:
    """
    Paralel retrieve + unique content filtering
    Birden fazla sorguyu aynÄ± anda iÅŸler ve duplicate iÃ§erikleri kaldÄ±rÄ±r
    """
    if not queries:
        return {"unique_contexts": [], "query_mapping": {}, "stats": {}}
    
    # Her query iÃ§in async task oluÅŸtur
    async def retrieve_single(query: str) -> tuple:
        try:
            if hasattr(RA, '_retrieve_async'):
                result = await RA._retrieve_async(query)
            else:
                # Fallback to sync version in thread pool
                loop = asyncio.get_event_loop()
                result = await loop.run_in_executor(None, RA.retrieve, query)
            return query, result
        except Exception as e:
            return query, f"Error retrieving '{query}': {str(e)}"
    
    # ğŸš€ PARALEL EXECUTION
    tasks = [retrieve_single(query) for query in queries]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # ğŸ” UNIQUE CONTENT FILTERING
    seen_contents = set()  # Raw text'leri track etmek iÃ§in
    unique_contexts = []   # Unique content'ler
    query_mapping = {}     # Hangi context hangi query'den geldi
    duplicate_count = 0
    
    for query, result in results:
        if isinstance(result, Exception):
            continue
            
        # Raw text'i normalize et (boÅŸluklarÄ± temizle)
        normalized_content = ' '.join(str(result).split())
        
        # ğŸ¯ DUPLICATE CHECK
        if normalized_content not in seen_contents:
            seen_contents.add(normalized_content)
            unique_contexts.append(str(result))
            
            # Ä°lk kez gÃ¶rÃ¼len content iÃ§in mapping yap
            query_mapping[len(unique_contexts) - 1] = {
                "primary_query": query,
                "content_preview": normalized_content[:100] + "..." if len(normalized_content) > 100 else normalized_content
            }
        else:
            duplicate_count += 1
            # Duplicate bulundu, hangi query'den geldiÄŸini mapping'e ekle
            for idx, mapping in query_mapping.items():
                # Bu content'in daha Ã¶nce kaydedilen versiyonunu bul
                if unique_contexts[idx] and ' '.join(unique_contexts[idx].split()) == normalized_content:
                    if "additional_queries" not in mapping:
                        mapping["additional_queries"] = []
                    mapping["additional_queries"].append(query)
                    break
    
    return {
        "unique_contexts": unique_contexts,
        "query_mapping": query_mapping, 
        "stats": {
            "total_queries": len(queries),
            "total_results": len(results),
            "unique_contents": len(unique_contexts),
            "duplicates_removed": duplicate_count,
            "deduplication_ratio": f"{duplicate_count}/{len(results)}" if results else "0/0"
        }
    }

async def process_stream_response(response_stream, session_id: str, client_id: str):
    """
    OpenAI stream response'unu iÅŸler ve tool calls varsa onlarÄ± yÃ¶netir
    """
    chat_history = []
    full_content = ""
    collected_tool_calls = {}
    
    async for chunk in response_stream:
        delta = chunk.choices[0].delta
        
        # Tool calls kontrolÃ¼ ve biriktirme
        if hasattr(delta, 'tool_calls') and delta.tool_calls:
            for tool_call_delta in delta.tool_calls:
                index = tool_call_delta.index
                
                # Ä°lk kez gÃ¶rÃ¼yorsak initialize et
                if index not in collected_tool_calls:
                    collected_tool_calls[index] = {
                        'id': '',
                        'type': 'function',
                        'function': {
                            'name': '',
                            'arguments': ''
                        }
                    }
                
                # ID varsa ekle
                if tool_call_delta.id:
                    collected_tool_calls[index]['id'] = tool_call_delta.id
                
                # Function bilgileri varsa ekle
                if tool_call_delta.function:
                    if tool_call_delta.function.name:
                        collected_tool_calls[index]['function']['name'] = tool_call_delta.function.name
                    if tool_call_delta.function.arguments:
                        collected_tool_calls[index]['function']['arguments'] += tool_call_delta.function.arguments
            
        # Content varsa ekle ve stream et
        if hasattr(delta, 'content') and delta.content:
            full_content += delta.content
            
            # Stream chunk'Ä±nÄ± gÃ¶nder
            await manager.send_stream_chunk({
                "type": "content_chunk",
                "content": delta.content
            }, client_id)
    
    # EÄŸer tool calls varsa iÅŸle
    if collected_tool_calls:
        # Tool calls tamamlandÄ±ÄŸÄ±nÄ± belirt
        await manager.send_stream_chunk({
            "type": "tool_calls_start",
            "message": "DokÃ¼manlardan bilgi aranÄ±yor..."
        }, client_id)
        
        # Collected tool calls'larÄ± proper format'a Ã§evir
        tool_calls_list = []
        for index, tool_call in collected_tool_calls.items():
            tool_calls_list.append({
                'id': tool_call['id'],
                'type': tool_call['type'],
                'function': tool_call['function']
            })
        
        # Tool calls'u chat history'ye ekle
        chat_history.append({
            "role": "assistant",
            "content": None,
            "tool_calls": tool_calls_list
        })
        
        # Her tool call'u iÅŸle
        for tool_call in tool_calls_list:
            function_name = tool_call['function']['name']
            
            try:
                function_args = json.loads(tool_call['function']['arguments'])
            except json.JSONDecodeError:
                print(f"JSON decode error for arguments: {tool_call['function']['arguments']}")
                continue
            
            call_id = tool_call['id']
            
            # ğŸš€ DOKUMAN ARAMA TOOL'U
            if function_name == "get_document_info":
                queries = function_args.get('queries', [])
                
                if not queries:
                    # Fallback: tek query varsa
                    single_query = function_args.get('query', '')
                    if single_query:
                        queries = [single_query]
                
                if not queries:
                    chat_history.append({
                        "role": "tool",
                        "content": json.dumps({
                            "error": "HiÃ§ arama sorgusu belirtilmedi"
                        }),
                        "tool_call_id": call_id
                    })
                    continue
                
                # Progress mesajÄ± gÃ¶nder
                query_text = ", ".join(queries[:3])  # Ä°lk 3 query'yi gÃ¶ster
                if len(queries) > 3:
                    query_text += f" ve {len(queries) - 3} tane daha"
                
                await manager.send_stream_chunk({
                    "type": "rag_search",
                    "message": f"'{query_text}' hakkÄ±nda dÃ¶kÃ¼manlardan bilgi aranÄ±yor..."
                }, client_id)
                
                try:
                    # ğŸš€ PARALEL RETRIEVE + UNIQUE FILTERING
                    start_time = asyncio.get_event_loop().time()
                    
                    retrieve_result = await batch_retrieve_async(queries)
                    unique_contexts = retrieve_result["unique_contexts"]
                    query_mapping = retrieve_result["query_mapping"]
                    stats = retrieve_result["stats"]
                    
                    end_time = asyncio.get_event_loop().time()
                    search_time = round((end_time - start_time) * 1000)  # milliseconds
                    
                    # ğŸ“Š DEDUPLICATION STATS
                    await manager.send_stream_chunk({
                        "type": "rag_complete",
                        "message": f"{stats['total_queries']} sorgu, {stats['unique_contents']} unique sonuÃ§ ({stats['duplicates_removed']} duplikat kaldÄ±rÄ±ldÄ±) - {search_time}ms"
                    }, client_id)
                    
                    # ğŸ”— UNIQUE CONTENT BÄ°RLEÅTÄ°RME
                    combined_context = "\n\n--- DÃ–KÃœMAN BÄ°LGÄ°LERÄ° ---\n\n"
                    
                    for i, context in enumerate(unique_contexts):
                        mapping = query_mapping.get(i, {})
                        primary_query = mapping.get("primary_query", f"Sorgu {i+1}")
                        additional_queries = mapping.get("additional_queries", [])
                        
                        # Hangi query'lerden geldiÄŸini belirt
                        source_info = f"Birincil sorgu: {primary_query}"
                        if additional_queries:
                            source_info += f" (AyrÄ±ca: {', '.join(additional_queries)})"
                        
                        combined_context += f"=== Bilgi BÃ¶lÃ¼mÃ¼ {i+1} ===\n"
                        combined_context += f"Kaynak: {source_info}\n"
                        combined_context += f"{context}\n\n"
                    
                    chat_history.append({
                        "role": "tool",
                        "content": json.dumps({
                            "queries": queries,
                            "unique_contexts": unique_contexts,
                            "combined_context": combined_context,
                            "query_mapping": query_mapping,
                            "stats": stats,
                            "search_time_ms": search_time
                        }),
                        "tool_call_id": call_id
                    })
                    
                except Exception as e:
                    chat_history.append({
                        "role": "tool", 
                        "content": json.dumps({
                            "queries": queries,
                            "error": f"DokÃ¼man arama sÄ±rasÄ±nda hata oluÅŸtu: {str(e)}"
                        }),
                        "tool_call_id": call_id
                    })

        # Tool calls tamamlandÄ±ktan sonra final response'u stream et
        await manager.send_stream_chunk({
            "type": "final_response_start",
            "message": "Cevap hazÄ±rlanÄ±yor..."
        }, client_id)
        
        # Final response iÃ§in yeni stream
        try:
            final_response = await openai_client.chat.completions.create(
                model="gpt-4o",
                messages=chat_history,
                temperature=0.7,
                stream=True
            )
            
            final_content = ""
            async for chunk in final_response:
                delta = chunk.choices[0].delta
                if hasattr(delta, 'content') and delta.content:
                    final_content += delta.content
                    await manager.send_stream_chunk({
                        "type": "content_chunk",
                        "content": delta.content
                    }, client_id)
            
            # Final message'Ä± history'ye ekle
            chat_history.append({
                "role": "assistant",
                "content": final_content
            })
            
            # Redis'e kaydet
            await redis_client.rpush(f"chat_history:{session_id}", json.dumps(chat_history[-1]))
            
        except Exception as e:
            print(f"Final response error: {e}")
            await manager.send_stream_chunk({
                "type": "content_chunk",
                "content": f"Cevap oluÅŸtururken bir hata oluÅŸtu: {str(e)}"
            }, client_id)
        
        # Stream tamamlandÄ±ÄŸÄ±nÄ± belirt
        await manager.send_stream_chunk({
            "type": "stream_end"
        }, client_id)
        
        return final_content if 'final_content' in locals() else ""
    
    else:
        # Tool calls yoksa direkt content'i kaydet
        if full_content:
            chat_history.append({
                "role": "assistant", 
                "content": full_content
            })
            
            # Redis'e kaydet
            await redis_client.rpush(f"chat_history:{session_id}", json.dumps(chat_history[-1]))
        
        # Stream tamamlandÄ±ÄŸÄ±nÄ± belirt
        await manager.send_stream_chunk({
            "type": "stream_end"
        }, client_id)
        
        return full_content

@app.websocket("/ws/{client_id}")
async def websocket_endpoint(websocket: WebSocket, client_id: str):
    """Ana WebSocket endpoint - kullanÄ±cÄ± ile etkileÅŸim"""
    session_id = str(uuid.uuid4())
    await manager.connect(websocket, client_id)

    try:
        while True:
            user_message = await websocket.receive_text()
            
            # KullanÄ±cÄ± mesajÄ±nÄ± Redis'e kaydet
            await redis_client.rpush(
                f"chat_history:{session_id}", 
                json.dumps({"role": "user", "content": user_message})
            )
            
            # Chat geÃ§miÅŸini al
            formatted_history = await redis_client.lrange(f"chat_history:{session_id}", 0, -1)
            formatted_history = [json.loads(msg) for msg in formatted_history]            
            
            # Stream baÅŸladÄ±ÄŸÄ±nÄ± belirt
            await manager.send_stream_chunk({
                "type": "stream_start"
            }, client_id)
            
            try:
                # OpenAI API stream Ã§aÄŸrÄ±sÄ±
                response_stream = await openai_client.chat.completions.create(
                    model="gpt-4.1",
                    messages=[
                        {
                            "role": "system",
                            "content": """
## ROL TANIMI
Sen akÄ±llÄ± bir soru-cevap asistanÄ±sÄ±n. KullanÄ±cÄ±larÄ±n sorularÄ±na yardÄ±mcÄ± olmak iÃ§in dokÃ¼man veri tabanÄ±nda arama yapabilir ve detaylÄ±, doÄŸru cevaplar verebilirsin.

## TEMEl GÃ–REVLERÄ°N
1. **Genel Sorular**: Basit, gÃ¼nlÃ¼k konuÅŸma tarzÄ± sorulara direkt cevap ver
2. **Spesifik Sorular**: DetaylÄ±, teknik veya Ã¶zel konularda bilgi gerektiren sorular iÃ§in dokÃ¼man aramasÄ± yap
3. **KarmaÅŸÄ±k Sorular**: Birden fazla konuyu kapsayan sorular iÃ§in sistematik arama stratejisi kullan

## KÄ°ÅÄ°LÄ°K Ã–ZELLÄ°KLERÄ°N
- **YardÄ±msever ve samimi**: KullanÄ±cÄ± dostu, anlaÅŸÄ±lÄ±r dil kullan
- **Dikkatli**: DoÄŸru bilgi vermeye odaklan
- **Verimli**: Gereksiz uzatmadan konuya odaklan
- **Åeffaf**: Bilgi kaynaklarÄ±nÄ± belirt, emin olmadÄ±ÄŸÄ±n durumlarda bunu sÃ¶yle

## CEVAP VERME STRATEJÄ°N

### Basit/Genel Sorularda:
- Direkt cevap ver (tool kullanma)
- KÄ±sa ve net ol
- GÃ¼nlÃ¼k dil kullan

Ã–rnek sorular:
- "Merhaba, nasÄ±lsÄ±n?"
- "TeÅŸekkÃ¼rler"
- "BugÃ¼n hava nasÄ±l?"

### Spesifik/Teknik Sorularda:
1. **Tool Kullan**: DokÃ¼manlardan bilgi ara
2. **Ã–zetle**: Bulunan bilgiyi kendi deyimlerinle aÃ§Ä±kla
3. **Kaynak Belirt**: Bilginin nereden geldiÄŸini sÃ¶yle

### KarmaÅŸÄ±k Sorularda:
1. **Sistematik YaklaÅŸÄ±m**: Soruyu parÃ§alara bÃ¶l
2. **Ã‡oklu Arama**: Her parÃ§a iÃ§in spesifik arama yap
3. **BÃ¼tÃ¼nleÅŸtir**: SonuÃ§larÄ± mantÄ±klÄ± bir ÅŸekilde birleÅŸtir

**Ã–rnek Query OluÅŸturma:**
KullanÄ±cÄ±: "X konusunda nasÄ±l baÅŸlarÄ±m ve hangi kaynaklarÄ± Ã¶nerirsin?"

Tool'a gÃ¶nderilecek queryler:
1. "X konusuna giriÅŸ rehberi"
2. "X Ã¶ÄŸrenme kaynaklarÄ±"
3. "X baÅŸlangÄ±Ã§ adÄ±mlarÄ±"
4. "X en iyi uygulamalar"

## TOOL KULLANIM KURALLARI

### Ne Zaman Tool KullanacaksÄ±n:
âœ… Teknik sorular
âœ… Spesifik bilgi isteyen sorular  
âœ… "NasÄ±l", "Ne", "Hangi" ile baÅŸlayan detay gerektiren sorular
âœ… Tarihsel bilgiler
âœ… ProsedÃ¼r/sÃ¼reÃ§ sorularÄ±

### Ne Zaman Tool KullanmayacaksÄ±n:
âŒ SelamlaÅŸma, teÅŸekkÃ¼r, kibarlÄ±k ifadeleri
âŒ "NasÄ±lsÄ±n?", "Ä°yi akÅŸamlar" gibi sohbet
âŒ Genel nezaket konuÅŸmalarÄ±

### Query OluÅŸturma Ä°puÃ§larÄ±:
- **Spesifik ol**: "genel bilgi" yerine "X'in tanÄ±mÄ±"
- **Anahtar kelimeler kullan**: Ã–nemli terimleri dahil et
- **KapsamlÄ± sorular iÃ§in parÃ§ala**: Her alt konu iÃ§in ayrÄ± query
- **Net ve kÄ±sa**: 2-8 kelime ideal

## Ä°LETÄ°ÅÄ°M TARZI
- GÃ¼nlÃ¼k konuÅŸma dili kullan
- KÄ±sa ve anlaÅŸÄ±lÄ±r cÃ¼mleler
- Jargondan kaÃ§Ä±n, basit aÃ§Ä±kla
- KullanÄ±cÄ±yÄ± dÃ¼ÅŸÃ¼nmeye teÅŸvik et
- "Sen ne dÃ¼ÅŸÃ¼nÃ¼yorsun?" tarzÄ± etkileÅŸim

## Ã–RNEK YAKLAÅIMLAR

### Basit Soru:
KullanÄ±cÄ±: "Merhaba!"
Sen: "Merhaba! Size nasÄ±l yardÄ±mcÄ± olabilirim?"

### Spesifik Soru:
KullanÄ±cÄ±: "Python'da liste nasÄ±l oluÅŸturulur?"
Sen: (Tool kullan) â†’ BulduÄŸun bilgiyi Ã¶zetle

### KarmaÅŸÄ±k Soru:
KullanÄ±cÄ±: "Web geliÅŸtirme nasÄ±l Ã¶ÄŸrenirim?"
Sen: (Ã‡oklu tool kullan: "web geliÅŸtirme baÅŸlangÄ±Ã§", "web programlama dilleri", "web geliÅŸtirme yol haritasÄ±")

## HATIRLATMALAR
- Her durumda yardÄ±mcÄ± olmaya Ã§alÄ±ÅŸ
- BilmediÄŸin konularda dÃ¼rÃ¼st ol
- Bulunan bilgiyi kendi sÃ¶zlerinle aÃ§Ä±kla
- KaynaklarÄ± direkt kopyalama, yorumla
- KullanÄ±cÄ±nÄ±n seviyesine uygun aÃ§Ä±kla

## KAÃ‡INILACAKLAR
âŒ Ã‡ok uzun akademik aÃ§Ä±klamalar
âŒ Gereksiz teknik detaylar
âŒ Belirsiz cevaplar
âŒ Kaynak bilgisini direkt yapÄ±ÅŸtÄ±rma
âŒ "Bu konuda bilgim yok" demek (Ã¶nce ara!)

Unutma: Sen bir bilgi kÃ¶prÃ¼sÃ¼sÃ¼n. KullanÄ±cÄ± ile dokÃ¼manlar arasÄ±nda akÄ±llÄ± bir baÄŸlantÄ± kuruyorsun.
                            """
                        }
                    ] + formatted_history,
                    tools=[
                        {
                            "type": "function",
                            "function": {
                                "name": "get_document_info",
                                "description": "DokÃ¼man veri tabanÄ±ndan bilgi aramak iÃ§in kullanÄ±lÄ±r. Spesifik, teknik veya detaylÄ± bilgi gerektiren sorular iÃ§in kullan. KarmaÅŸÄ±k sorular iÃ§in birden fazla arama sorgusu oluÅŸturabilirsin.",
                                "parameters": {
                                    "type": "object",
                                    "properties": {
                                        "queries": {
                                            "type": "array",
                                            "items": {
                                                "type": "string"
                                            },
                                            "description": "KullanÄ±cÄ±nÄ±n sorusuna cevap vermek iÃ§in dokÃ¼manlardan arama yapÄ±lacak sorgu listesi. KapsamlÄ± konular iÃ§in birden fazla spesifik sorgu oluÅŸtur. Her sorgu kÄ±sa ve odaklÄ± olmalÄ±.",
                                            "minItems": 1,
                                            "maxItems": 5
                                        }
                                    },
                                    "required": ["queries"]
                                }
                            }
                        },
                    ],
                    temperature=0.7,
                    stream=True,
                )
                
                # Stream response'u iÅŸle
                await process_stream_response(response_stream, session_id, client_id)
                
            except Exception as e:
                print(f"OpenAI API error: {e}")
                await manager.send_stream_chunk({
                    "type": "content_chunk",
                    "content": f"ÃœzgÃ¼nÃ¼m, bir hata oluÅŸtu: {str(e)}"
                }, client_id)
                await manager.send_stream_chunk({
                    "type": "stream_end"
                }, client_id)
            
    except WebSocketDisconnect:
        manager.disconnect(client_id)
    except Exception as e:
        print(f"WebSocket error: {e}")
        manager.disconnect(client_id)

# Chat geÃ§miÅŸini getiren endpoint
@app.get("/chat_history/{session_id}")
async def get_chat_history(session_id: str):
    """Belirli bir session'Ä±n chat geÃ§miÅŸini dÃ¶ndÃ¼rÃ¼r"""
    history = await redis_client.lrange(f"chat_history:{session_id}", 0, -1)
    return [json.loads(msg) for msg in history]

# Health check endpoint
@app.get("/health")
async def health_check():
    """Sistem saÄŸlÄ±k kontrolÃ¼"""
    try:
        # Redis baÄŸlantÄ±sÄ±nÄ± test et
        await redis_client.ping()
        redis_status = "healthy"
    except:
        redis_status = "unhealthy"
    
    # RAPTOR durumunu kontrol et
    raptor_status = "healthy" if RA and RA.tree else "unhealthy"
    
    return {
        "status": "healthy" if redis_status == "healthy" and raptor_status == "healthy" else "unhealthy",
        "redis": redis_status,
        "raptor": raptor_status,
        "active_connections": len(manager.active_connections)
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        app, 
        host="0.0.0.0", 
        port=8000,
        log_level="info",
        access_log=True
    )