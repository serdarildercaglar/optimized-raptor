
# ğŸŒ³ RAPTOR Production Suite

<div align="center">
![RAPTOR Logo](https://img.shields.io/badge/RAPTOR-Production%20Ready-green?style=for-the-badge)
![Python](https://img.shields.io/badge/Python-3.8+-blue?style=for-the-badge&logo=python)
![FastAPI](https://img.shields.io/badge/FastAPI-Modern-green?style=for-the-badge&logo=fastapi)
![Redis](https://img.shields.io/badge/Redis-Caching-red?style=for-the-badge&logo=redis)

**Enterprise-Grade Hierarchical RAG System**

[ğŸš€ Quick Start](#-quick-start) â€¢ [ğŸ“– Documentation](#-documentation) â€¢ [ğŸ”§ Configuration](#%EF%B8%8F-configuration) â€¢ [ğŸ“Š Monitoring](#-monitoring) â€¢ [â“ FAQ](#-faq)

</div>
---


---

## ğŸ“‹ Table of Contents

* [ğŸ¯ About](#-about)
* [â­ Features](#-features)
* [ğŸš€ Quick Start](#-quick-start)
* [ğŸ“ Project Structure](#-project-structure)
* [ğŸ“– Documentation](#-documentation)
* [ğŸ”§ Configuration](#%EF%B8%8F-configuration)
* [ğŸ“Š Monitoring](#-monitoring)
* [âš¡ Performance](#-performance)
* [â“ FAQ](#-faq)

---

## ğŸ¯ About

**RAPTOR Production Suite** is an enterprise-grade implementation of Stanford's RAPTOR (Recursive Abstractive Processing for Tree-Organized Retrieval) approach, designed for production environments with advanced caching, monitoring, and deployment capabilities.

### ğŸ¤” What Does It Do?

1. **Hierarchical Document Processing** : Creates multi-layer tree structures from documents
2. **Intelligent RAG System** : Provides context-aware question answering
3. **Real-time WebSocket API** : Streams responses with tool calls and progress tracking
4. **Production Monitoring** : Comprehensive metrics, health checks, and analytics
5. **Environment-based Configuration** : Development/Staging/Production settings

### ğŸ—ï¸ How It Works?

```
ğŸ“„ Documents â†’ ğŸŒ³ RAPTOR Tree â†’ ğŸ” Smart Retrieval â†’ ğŸ¤– AI Response â†’ ğŸ’¬ User
    â†“              â†“                â†“                â†“             â†“
  Chunks      Hierarchical     Context-Aware    Streaming      Real-time
            Summarization       Search         Response        Chat
```

---

## â­ Features

### ğŸš€ **Core RAPTOR Features**

* âœ…  **Hierarchical Document Understanding** : Multi-layer tree construction with clustering
* âœ…  **Async Processing** : Fully asynchronous operations with concurrent handling
* âœ…  **Advanced Caching** : Query similarity-based caching with Redis backend
* âœ…  **Adaptive Retrieval** : Smart parameter adjustment based on query characteristics
* âœ…  **Early Termination** : Performance optimization for faster responses

### ğŸ¢ **Production Features**

* âœ…  **Real-time WebSocket API** : Streaming responses with progress tracking
* âœ…  **Environment Management** : Development/Staging/Production configurations
* âœ…  **Health Monitoring** : System health checks and status endpoints
* âœ…  **Performance Analytics** : Real-time metrics and historical analysis
* âœ…  **Load Testing** : Built-in performance testing tools
* âœ…  **Redis Integration** : Caching and session management

### ğŸ“Š **Monitoring & Analytics**

* âœ…  **Live Metrics** : Real-time performance monitoring via HTTP endpoints
* âœ…  **Historical Data** : Performance trends and analytics
* âœ…  **Resource Tracking** : Memory, CPU, and cache efficiency monitoring
* âœ…  **Error Tracking** : Error logging and analysis
* âœ…  **Cache Management** : Memory and Redis cache monitoring

### ğŸ”§ **Technical Features**

* âœ…  **Multiple Embedding Models** : OpenAI, SentenceTransformers, Custom models
* âœ…  **Batch Processing** : Optimized embedding and summarization operations
* âœ…  **Memory Management** : Cache cleanup and garbage collection
* âœ…  **Configuration Management** : Environment-based settings with validation
* âœ…  **Docker Support** : Basic containerization support

---

## ğŸš€ Quick Start

### 1ï¸âƒ£ **Prerequisites**

```bash
# Python 3.8+ required
python --version

# Required for memory monitoring (optional)
pip install psutil
```

### 2ï¸âƒ£ **Installation**

```bash
# Clone repository
git clone <your-repo-url>
cd raptor-production

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements.txt
```

### 3ï¸âƒ£ **Environment Setup**

```bash
# Create environment file
cp .env.example .env

# Edit with your settings
nano .env
```

Required `.env` variables:

```env
OPENAI_API_KEY=sk-your-actual-openai-key-here
REDIS_PASSWORD=your-secure-password
RAPTOR_ENV=development
```

### 4ï¸âƒ£ **Build RAPTOR Tree**

```bash
# Prepare your document
echo "Your document content here..." > data.txt

# Build RAPTOR tree with production optimizations
python build-raptor-production.py data.txt --profile balanced
```

### 5ï¸âƒ£ **Start Server**

```bash
# Development server
python generic-qa-server.py

# Or production deployment with auto Redis setup
python deploy-raptor-production.py --env production
```

### 6ï¸âƒ£ **Test**

```bash
# Health check
curl http://localhost:8000/health

# Open web interface
open test.html  # In your browser
```

---

## ğŸ“ Project Structure

```
raptor-production/
â”œâ”€â”€ ğŸ“œ CORE FILES
â”‚   â”œâ”€â”€ README.md                       # This file
â”‚   â”œâ”€â”€ requirements.txt                # Python dependencies
â”‚   â”œâ”€â”€ .env.example                    # Environment template
â”‚   â””â”€â”€ data.txt                        # Sample document
â”‚
â”œâ”€â”€ ğŸŒ³ RAPTOR LIBRARY
â”‚   â”œâ”€â”€ raptor/
â”‚   â”‚   â”œâ”€â”€ tree_retriever.py           # Main retrieval system
â”‚   â”‚   â”œâ”€â”€ cluster_tree_builder.py     # Tree construction
â”‚   â”‚   â”œâ”€â”€ EmbeddingModels.py          # Embedding models
â”‚   â”‚   â”œâ”€â”€ tree_structures.py          # Tree data structures
â”‚   â”‚   â”œâ”€â”€ utils.py                    # Utility functions
â”‚   â”‚   â”œâ”€â”€ cluster_utils.py            # Clustering algorithms
â”‚   â”‚   â”œâ”€â”€ tree_builder.py             # Base tree builder
â”‚   â”‚   â”œâ”€â”€ Retrievers.py               # Base retriever interface
â”‚   â”‚   â”œâ”€â”€ QAModels.py                 # Question answering models
â”‚   â”‚   â”œâ”€â”€ SummarizationModels.py      # Summarization models
â”‚   â”‚   â”œâ”€â”€ FaissRetriever.py           # FAISS-based retrieval
â”‚   â”‚   â”œâ”€â”€ RetrievalAugmentation.py    # Main RA system
â”‚   â”‚   â””â”€â”€ __init__.py                 # Package initialization
â”‚
â”œâ”€â”€ ğŸš€ PRODUCTION SCRIPTS
â”‚   â”œâ”€â”€ generic-qa-server.py            # Main WebSocket server
â”‚   â”œâ”€â”€ build-raptor-production.py      # Production tree builder
â”‚   â”œâ”€â”€ production-config.py            # Configuration manager
â”‚   â”œâ”€â”€ deploy-raptor-production.py     # Deployment automation
â”‚   â””â”€â”€ performance-optimizer.py        # Load testing & optimization
â”‚
â”œâ”€â”€ âš™ï¸ CONFIGURATION
â”‚   â”œâ”€â”€ config/                         # Auto-generated config files
â”‚   â””â”€â”€ .env                            # Environment variables (create this)
â”‚
â”œâ”€â”€ ğŸ“Š WEB INTERFACE & RESULTS
â”‚   â”œâ”€â”€ test.html                       # Web chat interface
â”‚   â””â”€â”€ performance_results.json        # Performance test results
â”‚
â””â”€â”€ ğŸŒ³ RAPTOR DATA (auto-generated)
    â””â”€â”€ vectordb/
        â”œâ”€â”€ raptor-production           # Built tree file
        â””â”€â”€ raptor-production_metrics.json # Build metrics
```

---

## ğŸ“– Documentation

### ğŸ”§ **Building RAPTOR Trees**

#### Basic Build

```python
# Command line
python build-raptor-production.py your_document.txt

# Or programmatically
from build_raptor_production import build_raptor_production

result = build_raptor_production(
    data_path="your_document.txt",
    output_path="vectordb/my-tree"
)
```

#### Advanced Build with Configuration

```python
# Performance profiles: "speed", "balanced", "quality", "memory"
result = build_raptor_production(
    data_path="large_document.txt",
    output_path="vectordb/optimized-tree",
    performance_profile="speed",
    force_rebuild=True
)

print(f"Build time: {result['metrics']['total_time_seconds']:.1f}s")
print(f"Nodes created: {result['tree_stats']['nodes']}")
```

### ğŸ” **Using RAPTOR for Retrieval**

#### Basic Usage

```python
from raptor import RetrievalAugmentation, RetrievalAugmentationConfig
from raptor import CustomEmbeddingModel, GPT41SummarizationModel

# Configuration
config = RetrievalAugmentationConfig(
    embedding_model=CustomEmbeddingModel(),
    summarization_model=GPT41SummarizationModel(),
    enable_caching=True,
    enable_async=True
)

# Load tree
RA = RetrievalAugmentation(tree="vectordb/raptor-production", config=config)

# Retrieve context
context = RA.retrieve("What is this document about?")
print(context)
```

#### Advanced Retrieval

```python
# Async retrieval with custom parameters
import asyncio

async def advanced_retrieve():
    context, layer_info = await RA.retrieve_async(
        "Complex question requiring detailed analysis",
        top_k=10,
        max_tokens=5000,
        collapse_tree=False,
        return_layer_information=True
    )
    return context, layer_info

# Run async
context, layers = asyncio.run(advanced_retrieve())
```

### ğŸ¤– **Question Answering**

#### Basic QA

```python
# Simple question answering
answer = RA.answer_question("What are the main points?")
print(answer)
```

#### Batch Question Answering

```python
# Process multiple questions
questions = [
    "What is the main topic?",
    "What are the key findings?", 
    "What recommendations are made?"
]

# Async batch processing
answers = await RA.answer_questions_batch(questions, batch_size=5)
for i, answer in enumerate(answers):
    print(f"Q{i+1}: {answer}")
```

### ğŸŒ **WebSocket API Usage**

#### Web Interface

Open `test.html` in your browser for a full-featured chat interface.

#### Python WebSocket Client

```python
import asyncio
import websockets
import json

async def chat_client():
    uri = "ws://localhost:8000/ws/python_client"
  
    async with websockets.connect(uri) as websocket:
        # Send question
        await websocket.send("What are the main topics?")
      
        # Receive streaming response
        async for message in websocket:
            data = json.loads(message)
          
            if data['type'] == 'content_chunk':
                print(data['content'], end='', flush=True)
            elif data['type'] == 'stream_end':
                print("\n--- Response complete ---")
                break

# Run client
asyncio.run(chat_client())
```

---

## ğŸ”§ Configuration

### ğŸ“ **Environment Configurations**

The system automatically creates configuration files in `config/` directory:

#### Development (config/development.json)

```json
{
  "raptor": {
    "batch_size": 50,
    "num_layers": 3,
    "enable_metrics": false,
    "max_concurrent_operations": 4
  },
  "workers": 1,
  "monitoring": {
    "enable_prometheus": false,
    "log_level": "DEBUG"
  }
}
```

#### Production (config/production.json)

```json
{
  "raptor": {
    "batch_size": 150,
    "num_layers": 5,
    "max_concurrent_operations": 12,
    "cache_ttl": 7200,
    "enable_caching": true,
    "adaptive_retrieval": true
  },
  "workers": 4,
  "monitoring": {
    "log_level": "INFO"
  }
}
```

### âš™ï¸ **Configuration Management**

```python
from production_config import get_production_config

# Auto-detect environment from RAPTOR_ENV
config = get_production_config()

# Specific environment and profile
config = get_production_config("production", "speed")

# Override specific settings
config.raptor.batch_size = 200
config.workers = 8
```

### ğŸ”§ **Performance Profiles**

| Profile      | Speed  | Quality | Memory | Use Case                          |
| ------------ | ------ | ------- | ------ | --------------------------------- |
| `speed`    | â­â­â­ | â­â­    | â­â­   | Real-time applications            |
| `balanced` | â­â­   | â­â­â­  | â­â­   | General purpose                   |
| `quality`  | â­     | â­â­â­  | â­     | Research & analysis               |
| `memory`   | â­â­   | â­â­    | â­â­â­ | Resource-constrained environments |

---

## ğŸ“Š Monitoring

### ğŸ“ˆ **Available Metrics**

#### System Metrics

* Memory usage and peak consumption
* CPU utilization
* Request throughput and response times
* Error rates and success rates

#### RAPTOR-Specific Metrics

* Cache hit rates and efficiency
* Retrieval performance
* Tree query statistics
* Embedding processing times

#### Session Management

* Active WebSocket connections
* Chat session analytics
* User activity tracking

### ğŸ“Š **Metrics Endpoints**

```bash
# Real-time metrics
curl http://localhost:8000/metrics/live

# Historical performance data (last 24h)
curl http://localhost:8000/metrics/historical?time_range=24h

# Performance analytics
curl http://localhost:8000/analytics/performance?time_range=7d

# Memory usage details
curl http://localhost:8000/memory/usage

# Session management
curl http://localhost:8000/sessions/list?status=active&limit=50
```

### ğŸ”§ **Management Endpoints**

```bash
# Clear all caches
curl -X DELETE http://localhost:8000/cache/clear

# Optimize RAPTOR performance
curl -X POST http://localhost:8000/raptor/optimize

# Delete specific chat history
curl -X DELETE http://localhost:8000/chat_history/session_id
```

---

## âš¡ Performance

### ğŸ§ª **Load Testing**

```bash
# Run comprehensive performance test
python performance-optimizer.py \
    --server-url http://localhost:8000 \
    --websocket-url ws://localhost:8000/ws/test_client \
    --output results.json

# Run test and generate optimization
python performance-optimizer.py --optimize
```

### ğŸ“Š **Real Performance Results**

Based on actual testing with the included `performance_results.json`:

| Metric                      | HTTP API   | WebSocket Chat |
| --------------------------- | ---------- | -------------- |
| **Success Rate**      | 100%       | 54.5%          |
| **Avg Response Time** | 1.4s       | 0.8s           |
| **Throughput**        | 12.4 req/s | 0.3 conv/s     |
| **P95 Response Time** | 3.4s       | 3.3s           |
| **Memory Usage**      | ~15GB peak | ~15.6GB peak   |

### ğŸ“Š **Performance Optimization**

#### Manual Optimization

```python
# High-performance configuration
config = RetrievalAugmentationConfig(
    tb_batch_size=200,                    # Larger batches
    max_concurrent_operations=16,         # More concurrency
    tr_enable_caching=True,               # Enable caching
    tr_early_termination=True,            # Early termination
    cache_ttl=7200,                       # 2-hour cache
    tr_adaptive_retrieval=True            # Adaptive parameters
)
```

#### Memory Optimization

```python
# Memory-optimized configuration
config = RetrievalAugmentationConfig(
    tb_batch_size=50,                     # Smaller batches
    max_concurrent_operations=4,          # Less concurrency
    cache_ttl=1800,                       # 30-minute cache
    tr_top_k=5                           # Fewer results
)
```

---

## â“ FAQ

### ğŸ¤” **General Questions**

#### **Q: What is RAPTOR and how does it work?**

**A:** RAPTOR creates hierarchical tree structures from documents by recursively clustering and summarizing content at different abstraction levels. This enables better context understanding compared to traditional flat RAG systems.

#### **Q: What document sizes are supported?**

**A:**

* **Minimum** : 1KB (few paragraphs)
* **Optimal** : 10KB - 1MB
* **Maximum** : 100MB+ (depends on configuration)

#### **Q: What are the system requirements?**

**A:**

* **Minimum** : 4 CPU cores, 8GB RAM
* **Recommended** : 8+ CPU cores, 16GB+ RAM
* **GPU** : Optional (speeds up local embedding models)

### ğŸ”§ **Technical Questions**

#### **Q: How do I process multiple documents?**

**A:**

```python
documents = ["doc1.txt", "doc2.txt", "doc3.txt"]
for doc in documents:
    result = build_raptor_production(
        data_path=doc,
        output_path=f"vectordb/tree-{Path(doc).stem}"
    )
```

#### **Q: Can I use custom embedding models?**

**A:** Yes! Inherit from `BaseEmbeddingModel`:

```python
from raptor.EmbeddingModels import BaseEmbeddingModel

class MyCustomModel(BaseEmbeddingModel):
    def create_embedding(self, text):
        # Your implementation
        return embedding_vector
  
    async def create_embedding_async(self, text):
        # Async implementation
        return await self.create_embedding(text)
```

#### **Q: How do I scale for production?**

**A:**

```bash
# Set environment variables
export MAX_WORKERS=8
export RAPTOR_BATCH_SIZE=200
export RAPTOR_ENV=production

# Use production deployment
python deploy-raptor-production.py --env production
```

### ğŸš€ **Deployment Questions**

#### **Q: How do I monitor performance in production?**

**A:**

```bash
# Real-time system health
curl http://localhost:8000/health

# Live performance metrics
curl http://localhost:8000/metrics/live

# Memory usage analysis
curl http://localhost:8000/memory/usage

# Performance analytics dashboard
curl http://localhost:8000/analytics/performance
```

#### **Q: How do I handle Redis setup?**

**A:** The deployment script automatically handles Redis:

```bash
# Automatic Redis setup and RAPTOR deployment
python deploy-raptor-production.py --env production
```

#### **Q: How do I optimize memory usage?**

**A:**

```bash
# Use memory-optimized profile
python build-raptor-production.py data.txt --profile memory

# Or set environment variables
export RAPTOR_BATCH_SIZE=50
export MAX_CONCURRENT_OPERATIONS=4
```

### ğŸ’¡ **Performance Questions**

#### **Q: How can I improve response times?**

**A:**

1. **Enable Early Termination** : Use `--profile speed`
2. **Increase Concurrent Operations** : Set `MAX_CONCURRENT_OPERATIONS=16`
3. **Optimize Cache Settings** : Set longer `cache_ttl`
4. **Use Async Operations** : Enable `enable_async=True`

#### **Q: My WebSocket connections are failing. What should I do?**

**A:**

1. **Check Server Health** : `curl http://localhost:8000/health`
2. **Verify Redis** : Ensure Redis is running
3. **Check Logs** : Look for errors in console output
4. **Test Simple Connection** : Use the included `test.html`

### ğŸ”’ **Security Questions**

#### **Q: How do I secure API keys?**

**A:**

```bash
# Use environment variables
export OPENAI_API_KEY="your-key"

# For production, use secrets management
# Never commit keys to version control
```

#### **Q: How do I clear sensitive chat data?**

**A:**

```bash
# Clear all caches
curl -X DELETE http://localhost:8000/cache/clear

# Delete specific session
curl -X DELETE http://localhost:8000/chat_history/session_id
```

### ğŸ“ **Support & Help**

#### **Q: How do I troubleshoot issues?**

**A:**

1. **Check Health** : `curl http://localhost:8000/health`
2. **View Logs** : Check console output for errors
3. **Test Performance** : `python performance-optimizer.py`
4. **Verify Environment** : Check `.env` file settings

#### **Q: The performance tests show poor results. What should I do?**

**A:**

1. **Check System Resources** : Monitor memory and CPU usage
2. **Optimize Configuration** : Try different performance profiles
3. **Verify RAPTOR Tree** : Ensure tree was built successfully
4. **Check Network** : Verify WebSocket connectivity

---

## ğŸ¯ Best Practices

### ğŸ—ï¸ **Development Best Practices**

1. **Test with small documents first**
2. **Use development environment for testing**
3. **Monitor resource usage during development**
4. **Keep logs for debugging**
5. **Use appropriate performance profiles**

### ğŸš€ **Production Best Practices**

1. **Enable comprehensive monitoring**
2. **Set up automated health checks**
3. **Use Redis for optimal performance**
4. **Implement proper error handling**
5. **Regular cache cleanup and optimization**

### ğŸ”’ **Security Best Practices**

1. **Use environment variables for secrets**
2. **Implement session management**
3. **Monitor access patterns**
4. **Regular cache cleanup for privacy**
5. **Use secure Redis passwords**

---

<div align="center">
## ğŸ‰ Ready to Build Amazing RAG Applications!

**RAPTOR Production Suite provides a solid foundation for enterprise-grade hierarchical RAG systems.**

[â­ Star this repo]() â€¢ [ğŸ› Report Bug]() â€¢ [ğŸ’¡ Request Feature]() â€¢ [ğŸ“– Documentation]()

**Built for Production AI Applications**

</div>
