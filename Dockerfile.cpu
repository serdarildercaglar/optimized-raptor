# CPU-only optimized build
FROM python:3.11-slim as base

# Build argument for CPU-only mode
ARG CPU_ONLY=true

# Set environment variables for CPU optimization
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1

# CPU optimizations
ENV OMP_NUM_THREADS=4
ENV MKL_NUM_THREADS=4
ENV OPENBLAS_NUM_THREADS=4
ENV TOKENIZERS_PARALLELISM=false

# HuggingFace optimizations
ENV HF_HOME=/app/.cache/huggingface
ENV TRANSFORMERS_CACHE=/app/.cache/huggingface/hub
ENV HF_DATASETS_CACHE=/app/.cache/huggingface/datasets
ENV HF_HUB_DISABLE_SYMLINKS_WARNING=1

# Force CPU mode
ENV FORCE_CPU_MODE=true
ENV CUDA_VISIBLE_DEVICES=""

# Install system dependencies (lighter for CPU-only)
RUN apt-get update && apt-get install -y \
    curl \
    wget \
    git \
    build-essential \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

# Set working directory
WORKDIR /app

# Create cache directories
RUN mkdir -p /app/.cache/huggingface/hub && \
    mkdir -p /app/.cache/huggingface/datasets && \
    mkdir -p /app/logs && \
    mkdir -p /app/embedding_cache && \
    mkdir -p /app/query_cache && \
    mkdir -p /app/.local_model_cache

# Install Python dependencies in stages for better caching
COPY requirements.txt .

# Install PyTorch CPU-only version first
RUN pip install --no-cache-dir --upgrade pip setuptools wheel && \
    pip install --no-cache-dir torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu

# Install core ML dependencies (CPU optimized)
RUN pip install --no-cache-dir \
    transformers[torch] \
    sentence-transformers \
    scipy \
    scikit-learn \
    numpy

# Install remaining requirements
RUN pip install --no-cache-dir -r requirements.txt

# Install additional production dependencies
RUN pip install --no-cache-dir \
    uvicorn[standard] \
    psutil \
    prometheus_client \
    redis[hiredis] \
    python-multipart

# Create non-root user
RUN useradd --create-home --shell /bin/bash --uid 1000 raptor && \
    chown -R raptor:raptor /app

# Switch to non-root user
USER raptor

# Copy application code
COPY --chown=raptor:raptor . .

# Pre-warm Python imports (CPU version)
RUN python -c "
import torch
import transformers
import sentence_transformers
import numpy
import redis
import fastapi
import uvicorn
print('✅ Core imports successful (CPU mode)')
print(f'PyTorch CUDA available: {torch.cuda.is_available()}')
print('Running in CPU-only mode')
"

# Add CPU-optimized model pre-loading script
RUN echo '#!/bin/bash\n\
echo "🚀 Starting RAPTOR in CPU mode..."\n\
python -c "\n\
import os\n\
import sys\n\
import time\n\
import logging\n\
\n\
logging.basicConfig(level=logging.INFO)\n\
logger = logging.getLogger(__name__)\n\
\n\
def preload_models_cpu():\n\
    try:\n\
        logger.info('📥 Pre-loading models for CPU...')\n\
        from sentence_transformers import SentenceTransformer\n\
        import torch\n\
        \n\
        # Set CPU device for PyTorch\n\
        torch.set_num_threads(4)\n\
        \n\
        # CPU-friendly models\n\
        models = [\n\
            'sentence-transformers/multi-qa-mpnet-base-cos-v1',  # Smaller model first\n\
            'intfloat/multilingual-e5-large'  # Larger model\n\
        ]\n\
        \n\
        for model_name in models:\n\
            logger.info(f'Loading {model_name} on CPU...')\n\
            try:\n\
                model = SentenceTransformer(model_name, device='cpu')\n\
                # Test encoding\n\
                _ = model.encode('test')\n\
                logger.info(f'✅ {model_name} loaded successfully on CPU')\n\
                del model\n\
            except Exception as e:\n\
                logger.warning(f'⚠️ Failed to load {model_name}: {e}')\n\
                \n\
    except Exception as e:\n\
        logger.error(f'Model preloading error: {e}')\n\
\n\
if __name__ == '__main__':\n\
    preload_models_cpu()\n\
    logger.info('🎉 CPU model pre-loading completed')\n\
"\n\
\n\
echo "🚀 Starting RAPTOR server in CPU mode..."\n\
exec python generic-qa-server.py' > /app/entrypoint-cpu.sh && \
    chmod +x /app/entrypoint-cpu.sh

# Health check optimized for CPU
HEALTHCHECK --interval=90s --timeout=45s --start-period=900s --retries=15 \
    CMD python -c "\
import requests, sys, json; \
try: \
    response = requests.get('http://localhost:8000/health', timeout=45); \
    data = response.json(); \
    models_loaded = data.get('models', {}).get('models_loaded', False); \
    status = data.get('status', 'unhealthy'); \
    print(f'Health: {status}, Models: {models_loaded} (CPU mode)'); \
    sys.exit(0 if models_loaded and status in ['healthy', 'degraded'] else 1) \
except Exception as e: \
    print(f'Health check failed: {e}'); \
    sys.exit(1)"

# Expose port
EXPOSE 8000

# Use CPU-optimized entrypoint
ENTRYPOINT ["/app/entrypoint-cpu.sh"]